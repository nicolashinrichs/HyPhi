## HyPhi Example Code

For all examples below, for the import statements to work, a script utilizing these
functions needs to be located in the `software_module` directory of the HyPhi repo.
Most of the examples build on the previous ones and are meant to illustrate a typical
workflow when using this package.

### Dual-fNIRS Hyperscanning Preprocessing

Below, we demonstrate how to construct a time-varying sequence of weighted graphs
from dual-fNIRS, VR kinematics, and ECG signals. All curvature and entropy computations
are performed downstream using HyPhi.

```python
import numpy as np
import pandas as pd
import networkx as nx
from scipy.interpolate import CubicSpline
from scipy.stats import zscore

def rigorous_imputation(series, sampling_rate=10, max_gap_sec=2.0):
    ts = series.copy()
    max_gap = int(max_gap_sec * sampling_rate)
    isnan = ts.isna()
    valid_mask = np.ones(len(ts), dtype=bool)
    groups = (isnan != isnan.shift()).cumsum()

    for _, idx in isnan.groupby(groups).groups.items():
        if isnan.iloc[idx[0]]:
            if len(idx) <= max_gap:
                start, end = idx[0] - 1, idx[-1] + 1
                if start >= 0 and end < len(ts):
                    cs = CubicSpline([start, end],
                                     [ts.iloc[start], ts.iloc[end]])
                    ts.iloc[idx] = cs(idx)
            else:
                valid_mask[idx] = False

    return ts, valid_mask


def process_dyad_fnirs(df_s1, df_s2, channels):
    data, masks = {}, []
    for ch in channels:
        s1, m1 = rigorous_imputation(df_s1[ch])
        s2, m2 = rigorous_imputation(df_s2[ch])
        data[f"S1_{ch}"] = zscore(s1, nan_policy="omit")
        data[f"S2_{ch}"] = zscore(s2, nan_policy="omit")
        masks.append(m1 & m2)
    return pd.DataFrame(data), np.logical_and.reduce(masks)


def process_kinematics(vr_file, target_timestamps):
    df = pd.read_csv(vr_file)
    vr = pd.DataFrame(index=target_timestamps)

    cols = [
        "P1 head rotation x", "P1 head rotation y", "P1 head rotation z",
        "P2 head rotation x", "P2 head rotation y", "P2 head rotation z",
        "P1 hand displacement", "P2 hand displacement"
    ]

    for c in cols:
        vr[c] = np.interp(target_timestamps, df["Time"], df[c])

    for p in ["P1", "P2"]:
        rot = vr[[f"{p} head rotation {a}" for a in ["x", "y", "z"]]]
        vel = rot.diff().fillna(0)
        vr[f"{p}_HeadMotion"] = np.linalg.norm(vel.values, axis=1)

    return vr


def process_ecg(ecg_file, target_timestamps):
    df = pd.read_csv(ecg_file)
    ecg = pd.DataFrame(index=target_timestamps)
    for col in ["PNS index", "Stress index", "Mean HR"]:
        ecg[col] = np.interp(target_timestamps, df["Time"], df[col])
    return ecg


def build_graph_sequence(fnirs, vr, ecg,
                         window_size=150, step_size=10, nan_thresh=0.1):
    graphs, meta = [], []
    for i in range(0, len(fnirs) - window_size, step_size):
        w = fnirs.iloc[i:i + window_size]
        if w.isna().mean().mean() > nan_thresh:
            continue

        corr = w.corr().abs()
        np.fill_diagonal(corr.values, 0)
        graphs.append(nx.from_numpy_array(corr.values))

        meta.append({
            "t_start": fnirs.index[i],
            "t_end": fnirs.index[i + window_size - 1],
            "mean_PNS": ecg.iloc[i:i + window_size]["PNS index"].mean(),
            "mean_Stress": ecg.iloc[i:i + window_size]["Stress index"].mean(),
            "mean_hand_disp": vr.iloc[i:i + window_size]
                                .filter(like="hand displacement")
                                .mean().mean(),
            "mean_head_motion": vr.iloc[i:i + window_size]
                                .filter(like="HeadMotion")
                                .mean().mean()
        })

    return graphs, meta
